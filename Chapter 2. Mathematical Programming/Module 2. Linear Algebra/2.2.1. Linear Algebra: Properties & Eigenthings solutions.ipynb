{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.0 Linear Algebra: An Introduction\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Identity Matrices](#Identity-Matrices)\n",
    "- [Diagonal Matrices](#Diagonal-Matrices)\n",
    "- [Determinant & Matrix Inverse](#Determinant-&-Matrix-Inverse)\n",
    "- [Orthogonal Matrices](#Orthogonal-Matrices)\n",
    "- [Eigenvalues & Eigenvectors](#Eigenvalues-&-Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Matrices\n",
    "Now that we have covered matrix multiplication, we can cover some interesting properties and types of matrices. A crucial type of matrix is known as an __identity matrix__. Identity matrices must be __square matrices__, meaning they have as many rows as it has columns. By definition, the matrix product of any square matrix and an identity matrix with the same number of rows and columns is itself. This property makes it a useful matrix when dealing with the matrix inverse, as well as eigenvalues and eigenvectors, which we will cover shortly. Identity matrices, denoted as __I__, have 1s on their main diagonal, and 0s elsewhere as shown below:\n",
    "\n",
    "$$ \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}, \\; \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\0 & 0 & 1\\end{bmatrix}, \\;\n",
    "\\begin{bmatrix} 1 & 0 & ... & 0 \\\\\n",
    "                0 & 1 & ... & 0 \\\\\n",
    "                \\vdots &    & \\ddots &  \\\\\n",
    "                0 & 0 & ... & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A\\times I = I\\times A = A\n",
    "$$\n",
    "\n",
    "You can check out for yourself, and you will see that whatever square matrix you multiply with an identity matrix will give you the original matrix! Here we show how to generate an identity matrix in NumPy, along with proof that we get the original matrix when mutiplying with an identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix: [[1 2]\n",
      " [2 1]]\n",
      "2x2 identity matrix: [[1. 0.]\n",
      " [0. 1.]]\n",
      "Matrix product: [[1. 2.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Defining our matrix\n",
    "A = np.array([[1, 2], [2, 1]])\n",
    "\n",
    "I = np.eye(2,2)\n",
    "print(\"Original Matrix:\", A)\n",
    "print(\"2x2 identity matrix:\", I)\n",
    "print(\"Matrix product:\", np.matmul(A, I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal Matrices\n",
    "Another useful type of matrix is a __diagonal matrix__, where entries along the main diagonal are the only non-zero values. A general MxM is shown below:\n",
    "\n",
    "$$\n",
    "D = diag(a_{1}, a_{2}, ..., a_{M}) = \\begin{bmatrix} a_{1} & 0 & ... & 0 \\\\\n",
    "                      0 & a_{2} & ... & 0  \\\\\n",
    "                      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0 & 0 & ... & a_{M}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $a_{i}, i=1,...,M$ are scalar numbers.\n",
    "\n",
    "Diagonal matrices have interesting properties:\n",
    "- matrix multiplication order does not matter with diagonal matrices\n",
    "- the transpose of a diagonal matrix is itself\n",
    "- it scales the value of each row of the matrix by the respective diagonal component in the diagonal matrix\n",
    "\n",
    "Some examples of matrix multiplication with diagonal matrices are shown below:\n",
    "$$\n",
    "\\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\times \\begin{bmatrix}1 & 1 \\\\ 1 & 1\\end{bmatrix} = \\begin{bmatrix}2 & 2 \\\\ 3 & 3\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}2 & 0 \\\\ 0 & 4\\end{bmatrix}\\times \\begin{bmatrix}1 & 2 & 3\\\\ 1 & 2 & 3\\end{bmatrix} = \\begin{bmatrix}2 & 4 & 6 \\\\ 4 & 8 & 12\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "After understanding some of its properties, we can even treat an identity matrix as a diagonal matrix that scales each row by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinant & Matrix Inverse\n",
    "A __determinant__ is a function of a square matrix that reduces it to a single number. The determinant of a matrix A is denoted |A| or det(A). If A consists of one element a, then |A| = a. For a 2x2 matrix it is given by:\n",
    "\n",
    "$$A = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}, \\; det(A) = \\begin{vmatrix}a & b \\\\ c & d\\end{vmatrix} = ad-bc$$\n",
    "\n",
    "Finding the determinant of an NxN square matrix for N > 2 can be done by recursively deleting rows and columns to create successively smaller matrices until they are all 2 × 2 dimensions, and then applying the 2x2 definition above. The most basic technique is __expansion by row__, which we show here for a 3 × 3 matrix:\n",
    "\n",
    "$$A = \\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i\\end{bmatrix} \\; det(A) = a\\begin{vmatrix} e & f \\\\ h & i\\end{vmatrix} - b\\begin{vmatrix} d & f \\\\ g & i\\end{vmatrix} + c\\begin{vmatrix} d & e \\\\ g & h\\end{vmatrix}$$\n",
    "\n",
    "<img src=\"images/determinant.png\"\n",
    "     alt=\"matrix\"\n",
    "     style=\"display:block; margin-left:auto; margin-right:auto; width:50%\"/>\n",
    "\n",
    "In this case we are expanding by row 1, which means deleting row 1 by successively deleting columns 1, column 2, and column 3 to create three 2×2 matrices. The determinant of each smaller matrix is multiplied by the entry corresponding to the intersection of the deleted row and column. The expansion alternately adds and subtracts each successive determinant.\n",
    "\n",
    "\n",
    "We will generally use a computer for these calculations, especially for higher dimensions. So why do we need the determinant? Well, it is a useful tool in determining whether a square matrix has an __inverse__, and if it does, is also used in computing the matrix inverse.\n",
    "\n",
    "If we have a square matrix, A, its inverse, $\\mathbf{A^{-1}}$, is defined such that:\n",
    "$$ A\\times A^{-1} = A^{-1} \\times A = I $$\n",
    "\n",
    "The inverse of a square matrix is a matrix of the same dimension that when multiplied with the original matrix, returns the identity matrix, as shown above. Hence, we see the importance of identity matrices! In terms of linear transformations, it is the opposite transformation: it takes an ouput vector(s) and returns your input vector(s). However, not all square matrices have an inverse. Matrices that have no inverse are known as __singular matrices__, and have a determinant equal to 0. Thus, to determine whether a square matrix has an inverse, we simply compute its determinant! Some examples of this concept are shown below:\n",
    "$$A = \\begin{bmatrix}1 & 0 \\\\ 0 & 2\\end{bmatrix}, \\; det(A) = 2, \\; A^{-1} = \\begin{bmatrix}1 & 0 \\\\ 0 & \\frac{1}{2}\\end{bmatrix}, \\; A \\times A^{-1} = I$$\n",
    "\n",
    "$$B = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}, \\; det(B) = 0,\\; \\text{(singular, no inverse)} $$\n",
    "\n",
    "In general, we compute matrix determinants and inverses with different programs rather than by hand due to their complexity. Below, we show how to compute determinants and inverses with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:\n",
      "det(A) = 2.0\n",
      "Inverse of A: [[1.  0. ]\n",
      " [0.  0.5]]\n",
      "Matrix product: [[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "Case 2:\n",
      "det(B) = 0.0\n",
      "Inverse of B: inverse not existent\n"
     ]
    }
   ],
   "source": [
    "# Defining our matrices\n",
    "A = np.array([[1,0],[0,2]])\n",
    "B = np.array([[1,2],[2,4]])\n",
    "\n",
    "\n",
    "# Case 1: An inverse exists!\n",
    "A_det = np.linalg.det(A)\n",
    "A_inv = np.linalg.inv(A)\n",
    "product = np.matmul(A,A_inv)\n",
    "print(\"Case 1:\")\n",
    "print(\"det(A) =\",A_det)\n",
    "print(\"Inverse of A:\",A_inv)\n",
    "print(\"Matrix product:\",product)\n",
    "print()\n",
    "\n",
    "\n",
    "# Case 2: Singular matrix, inverse does NOT exist\n",
    "B_det = np.linalg.det(B)\n",
    "try:\n",
    "    B_inv = np.linalg.inv(B) # checking to see if Python finds an error\n",
    "except:\n",
    "    B_inv = \"inverse not existent\"\n",
    "\n",
    "print(\"Case 2:\")\n",
    "print(\"det(B) =\",B_det)\n",
    "print(\"Inverse of B:\",B_inv)"
   ]
  },
  {
   "source": [
    "It is important to note that __you can only compute the matrix inverse of a square matrix__. There are ways to get around this issue for rectangular matrices, but they won't be covered in the scope of this course."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Matrices\n",
    "Another type of matrix is an __orthogonal matrix__. Orthogonal matrices have the interesting product that their transpose is also their matrix inverse! In formal terms, given an orthogonal matrix A:\n",
    "\n",
    "$$ A \\times A^{T} = A^{T} \\times A = I \\implies A^{T} = A^{-1}$$\n",
    "\n",
    "This is yet another example of why it is useful to visualize matrices as composed of column/row vectors. In this case, a matrix is orthogonal if its columns and rows are orthogonal unit vectors (orthonormal). This means that the inner product of its rows with its columns will yield 1 if it's the same vector, and zero otherwise. Thus, to check if a matrix is orthogonal, we simply have to prove that the inner product of different columns is 0 and their lengths are equal to 1. An example of an orthogonal matrix is shown below.\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix} = \\begin{bmatrix} \\mathbf{v_{1}} & \\mathbf{v_{2}} \\end{bmatrix}, \\;\\; ||\\mathbf{v_{1}}|| = 1, ||\\mathbf{v_{2}}|| = 1,  \\mathbf{v_{1}} \\cdot \\mathbf{v_{2}} = 0 \\implies \\text{orthogonal} \n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\times A^{T} = \n",
    "\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix} \n",
    "\\times\n",
    "\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix} \n",
    " = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I\n",
    " $$\n",
    " \n",
    "Computing the inverse of a matrix is generally a complex process, which means that exploiting matrix orthogonality allows us to simplify manual calculations and reduce computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a function that checks whether a matrix is orthogonal with minimal NumPy\n",
    "def transpose(mat):\n",
    "    mat_transpose = [] # initialise transposed matrix\n",
    "    for idx2 in range(len(mat[0])): # iterate through columns\n",
    "        new_row = []\n",
    "        for idx1 in range(len(mat)): # iterate through rows\n",
    "            new_row.append(mat[idx1][idx2])\n",
    "        mat_transpose.append(new_row)\n",
    "    return mat_transpose\n",
    "\n",
    "def inner_product(v1, v2): # function that computes algebraic inner product of two vectors\n",
    "    product = 0\n",
    "    for value1, value2 in zip(v1, v2):\n",
    "        product += value1*value2\n",
    "    return product\n",
    "\n",
    "def matrix_product(mat1, mat2): # function that computes the matrix product between two matrices\n",
    "    result = []\n",
    "    mat2 = transpose(mat2)\n",
    "    for i in range(len(mat1)): # iterate over rows of first matrix\n",
    "        new_row = []\n",
    "        for j in range(len(mat2)): # iterate columns of the second matrix\n",
    "            product = inner_product(mat1[i][:], mat2[j][:]) # inner product between row and column vectors\n",
    "            new_row.append(product)\n",
    "        result.append(new_row)\n",
    "    return result\n",
    "\n",
    "## Code function here (I recommend using np.isclose to check if two floats are equal)\n",
    "def is_orthogonal(mat):\n",
    "    trans_mat = transpose(mat)\n",
    "    product = matrix_product(mat, trans_mat)\n",
    "    orthogonal = True\n",
    "    for i, row in enumerate(product):\n",
    "        for j, entry in enumerate(row):\n",
    "            if i==j:\n",
    "                if not np.isclose(entry, 1):\n",
    "                    orthogonal = False\n",
    "            else:\n",
    "                if not np.isclose(entry, 0):\n",
    "                    orthogonal = False\n",
    "    return orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues & Eigenvectors\n",
    "We will now put together all we have covered together to understand one of the most important concepts in linear algebra: __eigenvalues__ and __eigenvectors__. Given a square matrix, A, its eigenvector(s), denoted by $\\vec{\\mathbf{v_{i}}}$ is/are vector(s) such that:\n",
    "\n",
    "$$A\\vec{\\mathbf{v_{i}}} = \\lambda_{i}\\vec{\\mathbf{v_{i}}}$$\n",
    "\n",
    "Where $\\lambda_{i}$ is the corresponding eigenvalue, a scalar.\n",
    "\n",
    "__Eigenvectors are any possible non-zero solutions for__ $\\mathbf{\\vec{v_{i}}}$ __the equation above__. What does this actually mean? Well, this is one of the instances where it makes sense to visualize a matrix as a linear transformation. As we mentioned before, we can apply a linear transformation to any vector with the appropriate number of components, and for a few vectors, the linear transformation only scales them by a given scalar value (their eigenvalue). These are the eigenvectors of that matrix.\n",
    "\n",
    "So how do we calculate the eigenvalues and eigenvectors of a matrix? Some of you have probably already guessed that a vector with all zero components is always an eigenvector. This is known as a __trivial solution__, and since it applies to all matrices regardless of the corresponding eigenvalue, it doesn't tell us about any interesting properties of individual matrices. We are, therefore, interested in the __non-trivial solution(s)__ for the eigenvectors of a matrix. We can rewrite the general expression in the form below, which allows us to do the following:\n",
    "\n",
    "$$A\\vec{\\mathbf{v_{i}}} = \\lambda_{i}I\\vec{\\mathbf{v_{i}}}$$\n",
    "$$(A - \\lambda_{i}I)\\vec{\\mathbf{v_{i}}} = 0 $$\n",
    "\n",
    "Some of you may be thinking: \"We can multiply both sides of the equation by $(A - \\lambda_{i}I)^{-1}$, but this results in the trivial solution we had intuitively found before. So what do we do now?\". Well, for $\\vec{\\mathbf{v_{i}}}$ to be a non-zero vector, $(A - \\lambda_{i}I)$ must be singular, meaning it CANNOT have an inverse. This is why there are so few \"exceptions\" that lead to the few eigenvectors of the matrix out of the infinitely many other vectors in the same vector space. Going one step further, we can use the determinant condition we came across before to solve for the eigenvalues of the matrix:\n",
    "\n",
    "$$det(A - \\lambda_{i}I) = 0$$\n",
    "\n",
    "If we keep $\\lambda_{i}$ as a variable, the expression above gives us a polynomial equation, known as the __characteristic polynomial__ of the matrix, and solving it will give you the eigenvalues of the matrix! To find their corresponding eigenvectors, we plug each eigenvalue into the original expression and solve for the vector. A simple, 2x2 example is shown below:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\implies \n",
    "A - \\lambda_{i}I = \\begin{bmatrix} 1-\\lambda_{i} & 2 \\\\ 2 & 1-\\lambda_{i} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "det(A - \\lambda_{i}I) = 1 - 2 \\lambda_{i} + \\lambda_{i}^{2} - 4 = \\lambda_{i}^{2} - 2 \\lambda_{i} - 3 = 0\n",
    "$$\n",
    "\n",
    "$$ \\lambda_{1} = -1, \\lambda_{2} = 3$$\n",
    "\n",
    "Now we can find the respective eigenvectors of each eigenvalue. We will start with the eigenvector of $\\lambda_{1}$:\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix}v_{x} \\\\ v_{y}\\end{bmatrix} = \n",
    "\\begin{bmatrix}-v_{x} \\\\ -v_{y}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ v_{x} + 2v_{y} = -v_{x}$$\n",
    "$$ 2v_{x} + v_{y} = -v_{y}$$\n",
    "This means that the corresponding eigenvectors of $\\lambda_{1}$ is any vector such that $v_{x} = -v_{y}$. We can generalize these to obtain our eigenvectors (which are all scalar multiples of each other):\n",
    "\n",
    "$$\\mathbf{\\vec{v_{1}}} = t\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}, \\text{where } t \\in \\Re$$\n",
    "\n",
    "The same procedure can be applied for the second eigenvalue, resulting in the following eigenvector:\n",
    "$$\\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix}v_{x} \\\\ v_{y}\\end{bmatrix} = \n",
    "\\begin{bmatrix}3v_{x} \\\\ 3v_{y}\\end{bmatrix}$$\n",
    "\n",
    "$$ v_{x} + 2v_{y} = 3v_{x}$$\n",
    "$$ 2v_{x} + v_{y} = 3v_{y}$$\n",
    "$$v_{x}=v{y}$$\n",
    "$$\\mathbf{\\vec{v_{2}}} = \\alpha\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\text{where } \\alpha \\in \\Re$$\n",
    "\n",
    "And that's it! This procedure can be applied to a square matrix of any size. However, as you can see from the 2x2 case, even a simple example can be time-consuming. For larger matrices, it is highly impractical to manually compute the eigenvalues and eigenvectors of the matrix, so, as usual, we write programs to do the hard stuff for us. Below we show how to use NumPy to calculate the eigenvalues/eigenvectors of any square matrix. We will use _np.linalg.eig(  ),_ which returns the only possible eigenvector of unit length. In reality, any scalar multiple of this vector is also a valid solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 3. -1.]\n",
      "Eigenvectors: [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2x2 matrix\n",
    "A = np.array([[1,2],[2,1]])\n",
    "w,v = np.linalg.eig(A)\n",
    "print(\"Eigenvalues:\",w)\n",
    "print(\"Eigenvectors:\",v)"
   ]
  },
  {
   "source": [
    "Eigenvalues and eigenvectors are very representative of the given linear transformation, and provide us with tools to analyse the behaviour of the transformation, decompose it into a simpler form, and much more!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "\n",
    "__Question 1__: Using NumPy, write a function named is_inverse() that:\n",
    "- Takes two matrices (of type ndarray) as input\n",
    "- Returns True if the one matrix is the inverse of the other, False otherwise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}