{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.0 Quantifying Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- [Introduction to Probability Theory](#Introduction-to-Probability-Theory)\n",
    "- [Randomness](#Randomness)\n",
    "- [Probability Distributions](#Probability-Distributions)\n",
    "- [Measures of Central Tendency](#Measures-of-Central-Tendency)\n",
    "- [Measures of Dispersion](#Measures-of-Dispersion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probability__ is a measure of how likely an outcome is to occur given all other possible outcomes and the given circumstances. Therefore, when dealing with random variables (soon to be discussed), we do not concern ourselves with __what will happen__, but instead with __the probability of given outcome(s), also known as events, occuring__. __Experimental probability__ is formally defined as:\n",
    "\n",
    "$$ \\text{Probability of an outcome} = \\frac{\\text{Number of wanted outcomes}}{\\text{Total number of outcomes}} $$\n",
    "\n",
    "Hence, from a given sample, we simply divide the number of observations that meet the given conditions at hand by the total number of observations in the sample. We can also consider what is known as __theoretical probability__ which is what is expected to happen.\n",
    "\n",
    "Let's take a look at an example of throwing a die, where the outcome is the number of dots. To calculate the theoretical probability of an outcome, we need to determine the __number of wanted outcomes__, the __number of possible outcomes__, and their respective likelihoods of taking place. Since there 6 possible numbers of dots that we can throw, they are the number of possible outcomes. Within the range of 6 possible outcomes, each number occurs exactly once. If we assume that each outcome is equally likely, we get the theoretical probability $P = \\frac{1}{6}$ for each possible number of dots.\n",
    "\n",
    "Intuitively, we know that the probability of something occurring has to be somewhere between 0, where the outcome _cannot_ occur, and 1, where the outcome _will_ occur, where everything else is somewhere in between. We can apply probability in the context of the likelihood of a random variable taking a value or being within a range of values.<br>\n",
    "\n",
    "From what is known as a frequentist approach, the more and more observations we have in our sample, the more our experimental probabilities will tend to the theoretical probabilities. From the previous analysis, we saw that the theoretical probability of throwing a 1 is $\\frac{1}{6}$. Let's simulate this scenario! Below we use the random module from NumPy to generate random dice throws and plots the experimental probability vs. number of throws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "dice_throws = np.random.randint(1, 7, size=1000) # returns random integers within the specified range\n",
    "desired_outcome_count = 0\n",
    "probabilities = []\n",
    "for idx, throw in enumerate(dice_throws):\n",
    "    if throw == 1:\n",
    "        desired_outcome_count += 1\n",
    "    probabilities.append(desired_outcome_count/(idx+1))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.linspace(1, len(dice_throws)), y=probabilities, name='Experimental'))\n",
    "fig.add_trace(go.Scatter(x=np.linspace(1, len(dice_throws)), y=np.ones_like(dice_throws)*(1/6), name='Theoretical'))\n",
    "fig.update_layout(\n",
    "    title='Probability of Throwing a 1',\n",
    "    xaxis_title=\"Number of Throws\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    yaxis_range=[0, 1]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "We see that the more throws we add on, the closer our experimental probability will be to the theoretical probability. As we have previously said, probability is used to deal with uncertainty and randomness. But what does that even mean?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability Theory deals with the quantification of our degree of uncertainty. Its main\n",
    "object of interest are random entities and, in particular, random variables. A __random variable is__ a defined __event__, whose outcome value is not always known, such as the result of an experiment.\n",
    "\n",
    "For example, if we take as an experiment the throwing of a die, we can define a random variable as the outcome of the throw. Random Variables are usually denoted with a capital letter, such as $X$. Therefore, we can write the probability of observing a given outcome, $x$, is given by $P(X=x)$. <br>\n",
    "\n",
    "We distinguish between two types of random variables, namely __discrete__ or __continuous__. __Discrete random variables__ can only take certain numbers as values. This is the case in our dice throwing experiment as the outcome can only take on the values within the set $\\{1, 2, 3, 4, 5, 6\\}$.  __Continuous random variables__ on the other hand can take any value within a range. The height of a person is an example of a continuous random variables as it can be any value in the certain range of heights, for example between 0.50m and 2.50m and could be determined as 1.65457m. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we understand the context of what we are analysing, we can use what is known as a __probability distribution__, which is just a function that maps a unique probability to every possible outcome. As a probability distribution accounts for all possible outcomes of the random variable, the sum of the probabilities given by the distribution will always be equal to 1. Consider the example of the probability distribution of the throw of a die below:\n",
    "\n",
    "| X    | 1             | 2             | 3             | 4             | 5             | 6             |\n",
    "|------|---------------|---------------|---------------|---------------|---------------|---------------|\n",
    "| P(X) | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ |\n",
    "\n",
    "We see that for all possible outcomes of the throw, the probability distribution assigns a probability. In this case, we have assigned theoretical probabilities, but the same can be done with experimental probabilities. The number of time an outcome is observed in statistics is known as the __frequency__ (things that happen more, are considered more _frequent_).\n",
    "\n",
    "## Expectation\n",
    "When we talk about random variables, we are usually interested in their __expected value__. The expected value $E$ can be interpreted as the weighted average value of the random variable. It is a single value that aims to describe the _center_ of the possible values a random variable can take. It is computed as the sum of multiplying each value in $X$ with its probability s.t. each value is weighted by its probability:<br>\n",
    "\n",
    "$$E(X) = \\mu = \\sum_{x \\in X} xP(X=x)$$\n",
    "\n",
    "This means that, in the case of the die throw with the probability distribution described by the above table, the expected value of $X$ is 3.5.\n",
    "\n",
    "If we know how our data will be distributed, then we also know how our data will be spread in the long-run. Hence, we can also compute what is referred to as the __population variance__.\n",
    "\n",
    "$$var(X) = \\sigma^{2} = \\sum_{x \\in X} (x-\\mu)^{2}P(X=x)$$\n",
    "\n",
    "This formula shows how the population variance is just a weighted average of the squared deviation from the population mean. The further away from the expected value we are on average, the larger the variance will be. Each possible outcome is weighted by its probability, since if it happens more times relative to other outcomes, it will have a more significant contribution to the outcome.\n",
    "\n",
    "Hence, when working under the assumption of a probability distribution we can compute the expected value of the mean, also referred to as the __population mean__, as well as the population variance. This means that, if we were to compute the experimental average and experimental variance (for which we show a formula below) of all the observed throws of a die as the number of throws gets larger and larger, these quantities will get closer and closer to their theoretical values. This is, creatively enough, known as the __law of large numbers__. Given this relationship, we can use theoretical probability distributions to help us understand the long-term behaviour of random variables! Consider below, where we update the sample mean from the simulated die throws.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the average for the random variable $X$ (number of eyes of one die roll) using Python!\n",
    "\n",
    "import numpy as np\n",
    "samples = []\n",
    "means = []\n",
    "#Our possible values:\n",
    "values = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "#let's roll the dice 10 times!\n",
    "rolls = 10000\n",
    "for i in range(rolls):\n",
    "    #the np.random.choice function assumes a uniform distribution, so all values with have P=1/6 without specifying it:\n",
    "    sample = np.random.choice(values)\n",
    "    samples.append(sample)\n",
    "    means.append(np.mean(samples))\n",
    "print(np.mean(samples))\n",
    "\n",
    "#Try to see how the average changes when we increase the number of times that we roll the die!\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.linspace(1, len(samples)), y=means, name='Experimental'))\n",
    "fig.add_trace(go.Scatter(x=np.linspace(1, len(samples)), y=np.ones_like(dice_throws)*3.5, name='Theoretical'))\n",
    "fig.update_layout(\n",
    "    title='Sample mean of Dice Throws',\n",
    "    xaxis_title=\"Number of Throws\",\n",
    "    yaxis_title=\"Mean\",\n",
    "    # yaxis_range=[0, 5]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Central Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just looked at the benefits of analysing random variables from a theoretical perspective. But we can't always make assumptions about the underlying distribution, so we also need to be able to understand random variables from experimental data alone. Given a dataset, we do not get much insight from looking at each datapoint individually, but from the descriptive measures. Some of them can be grouped as __measures of central tendency__, which are single values that describe the center of the data. These measures tell us where the most values within our data lie. In a plot, we can visualise these central points as the point where most of the data points cluster together. \n",
    "\n",
    "## Mean\n",
    "The most common measure of central tendency is the __sample mean $\\bar{x}$__, the sum of all values divided by the number of values $n$:\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i}^{n} x_{i}$$\n",
    "While it is a very popular measure, it is also very sensitive to __outliers__. Outliers are data points that are very far from the other values in a dataset. For example if we measure the height of people in a population, which are within a general range of 1.50m - 1.90m, we might record a value of a person with height 2.20m. This single case will heavily influence the mean of our dataset, which we should consider when choosing a measure of central tendency.\n",
    "\n",
    "## Median\n",
    "If we sort our data values, the __median__ is simply the middle value that separates the data into two equal-sized halves. This works fine, if we have 11 data points as the median will split the sides into 5 data points each. With an even number of data points, we would take the two middle values and average them. The advantage of the median is that it is not affected by outliers.\n",
    "\n",
    "## Mode\n",
    "The __mode__ is the value that occurs most frequently, i.e. the most popular value. It can tell us a lot about the data, but we should interpret it with caution. Particularly when there is more than one most frequent value, it is difficult to choose one as the mode. It is also not a reliable measure of central tendency if it is far away from the range of the remaining values.\n",
    "\n",
    "Write functions using standard Python to calculate the mean, median and mode of the height data below. Use Plotly to plot a histogram of the data shown below<br> \n",
    "\n",
    "| ID | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Height | 1.64 | 1.71 | 1.71 | 1.5 | 2.15 | 1.53 | 1.85 | 1.7 | 1.85 | 1.85 | 1.77 |"
   ]
  },
  {
   "source": [
    "## Coding up functions\n",
    "def mean(X):\n",
    "    pass\n",
    "def mode(X):\n",
    "    pass\n",
    "def median(X):\n",
    "    pass\n",
    "\n",
    "# Finding values for the exercise\n",
    "heights = [1.64, 1.71, 1.71, 1.5, 2.15, 1.53, 1.85, 1.7, 1.85, 1.85, 1.77]\n",
    "print('Mean: {}'.format(mean(heights)))\n",
    "print('Mode: {}'.format(mode(heights)))\n",
    "print('Median: {}'.format(median(heights)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While measures of central tendency describe the center of the data, we are also interested in how the rest of the data is scattered around the center. Therefore, we can use the following __measures of dispersion__:\n",
    "\n",
    "## Range\n",
    "As a first statistic, we should look at the __range__ of our values. The range is simply the difference between the maximum and minimum value. It is specifically useful to consider the range when adding new data as we can detect outliers, if they lie outside of the range.\n",
    "$$ \\text{Range} = x_{max} - x_{min} $$\n",
    "Due to the simplicity of the calculation, range is not a robust measure spread, and is very sensitive to outliers.\n",
    "\n",
    "\n",
    "## Quartiles\n",
    "Similar to how the median divides the data into 2 halves, __quartiles__ divide the data into 4 quarters. You might also come across different notations. The __first quartile__ corresponds to the __median__ of the lower half of the dataset. It is also known as the __25th percentile__ or __lower quartile__ and often abbreviated as __$Q_1$__. Similarly, the __upper quartile__ or __third quartile__ (__$Q_3$__) corresponds to the __75th percentile__ and denotes the median of the upper half of the dataset.\n",
    "\n",
    "## Interquartile range\n",
    "Unsurprisingly,  the __interquartile range__ corresponds to the range of values between the lower and upper quartile. While the __range__ over the entire dataset is the difference between the overall max and min values, the interquartile range denotes the difference between the the quartiles $Q_1$ and $Q_3$.\n",
    "$$ \\text{Interquartile range} = Q_3 - Q_1$$\n",
    "The interquartile range is a way to describe the data values without being skewed by outliers as they are not included here unlike in the overall range.\n",
    "<br>\n",
    "<img src=\"images/quartiles.png\" width=350 height=400 align=\"center\"/> <br>\n",
    "Quartiles and interquartile range within a normal probability distribution. ([Source](https://www.researchgate.net/figure/Relationship-of-quartiles-and-inter-quartile-range-Legends-Q-1-first-quartile-Q-3_fig2_324532937))<br>\n",
    "\n",
    "## Variance\n",
    "The __variance $\\sigma^2$__ measures how spread out from the mean the overall data is. Therefore, we first have to calculate the __mean $\\mu$__ and subtract from each value. In order to ensure positivity, we square the resulting values and then sum them together. This results in the __sum of squares__. To derive the __variance__ from the sum of squares, we simpy divide by the number of data points $n$. Based on this formula, can also explain the variance as the mean of squared differences of each data point from the mean.\n",
    "$$ \\sigma^2 = \\frac{(\\sum x - \\mu)^2}{n} $$\n",
    "A large variance indicates that the values are spread out far. By squaring the values, large values such as outliers will affect the variance a lot. It is also difficult to relate the variance to the data as the variance is based on squared values and therefore no longer in the same unit as the original data.\n",
    "\n",
    "## Standard deviation\n",
    "The __standard deviation $\\sigma$__ alleviates this problem by taking the square-root of the variance.\n",
    "$$ \\sigma = \\sqrt{\\frac{(\\sum x - \\mu)^2}{n}} $$\n",
    "\n",
    "## Exercise 3\n",
    "Write your own function for range, and using the previously written function for median, write a function that computes the interquartile range. Additionally, determine the quartiles in the height dataset and compare range, interquartile range and standard deviation. Which measure would you choose for which purpose?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code up our functions for these measurs\n",
    "def range(X):\n",
    "    pass\n",
    "def IQR(X):\n",
    "    pass\n",
    "def std(X):\n",
    "    pass\n",
    "\n",
    "    # Finding values for the exercise\n",
    "heights = [1.64, 1.71, 1.71, 1.5, 2.15, 1.53, 1.85, 1.7, 1.85, 1.85, 1.77]\n",
    "print('Range: {}'.format(range(heights)))\n",
    "print('IQR: {}'.format(IQR(heights)))\n",
    "print('Standard deviation: {}'.format(std(heights)))"
   ]
  },
  {
   "source": [
    "## Statistics with Coding Examples\n",
    "\n",
    "Now that you understand the basic measures in probability theory and have calculated them by hand, we will look into some python functions that will do this automatically for us! These come in very handy once we start to work with larger datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the iris dataset (more information on https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n",
    "iris = datasets.load_iris()\n",
    "#Select the first feature (RV) to inspect (sepal length)\n",
    "sepal_length = iris.data[:,0]\n",
    "\n",
    "#Calculate the measures of central tendency\n",
    "#Calculate the mean of the mean of the first feature:\n",
    "mean = np.mean(sepal_length)\n",
    "print('Mean:', mean)\n",
    "\n",
    "#Calculate the mode\n",
    "mode = stats.mode(sepal_length)\n",
    "print('Mode:', mode) #returns the mode as well as the number of times it occurred\n",
    "\n",
    "#Calculate median\n",
    "median = np.median(sepal_length)\n",
    "print('Median:', median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate measures of spread\n",
    "variance = np.var(sepal_length)\n",
    "print(variance)\n",
    "\n",
    "#Calculate standard deviation\n",
    "std = np.std(sepal_length)\n",
    "print(std)\n",
    "\n",
    "#Plot the interquartile range\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=sepal_length, name='Sepal Length'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram\n",
    "fig = go.Figure(data=[go.Histogram(x=sepal_length)])\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "# Challenges\n",
    "\n",
    "Question 1:\n",
    "- Write a function that takes in a list of possible outcomes of a random variable, a list of their probability distribution, and returns the expected value of the random variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}