{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Basics of Probability Theory\n",
    "* Frequentist vs. Bayesian Inference\n",
    "* Measures of Central Tendency\n",
    "* Measures of Dispersion/Spread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probability__ is a measure of how likely an outcome is to occur given all other possible outcomes and the given circumstances. Therefore, when dealing with random variables, we do not concern ourselves with __what will happen__, but instead with __the probability of given outcome(s), also known as events, occuring__. Probability is formally defined as:\n",
    "\n",
    "$$ \\text{Probability of an outcome} = \\frac{\\text{Number of wanted outcomes}}{\\text{Number of possible outcomes}} $$\n",
    "\n",
    "Intuitively, we know that the probability of something occurring has to be somewhere between 0, where the outcome _cannot_ occur, and 1, where the outcome _will_ occur, where everything else is somewhere in between. We can apply probability in the context of the likelihood of a random variable taking a value or being within a range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability Theory deals with the quantification of our degree of uncertainty. Its main\n",
    "object of interest are random entities and, in particular, random variables. A __random variable is__ a defined __event__, whose outcome value is not always known, such as the result of an experiment. For example, if we take as an experiment the throwing of two dice, we can define a random variable as the sum of boths throws. They are usually denoted with a capital letter, such as $X$. Therefore, we can write the probability that the sum is 10 as $P(X=10)$. <br>\n",
    "\n",
    "Each random variable underlies a probability distribution where the probabilities of each outcome are defined. In the case that we have two fair dice, we can assume a __uniform probability distribution__. In a uniform probability distribution, all events have the same probability. Since our random variable can have 11 events, i.e. we can have sum value in the range $\\{2,...,12\\}$, the probability of each event is $P =  (\\frac{1}{11})$. We will discuss different types of proability distributions in [Chapter 2.4].\n",
    "\n",
    "When we talk about random variables, we are usually interested in their __expected value__. The expected value $E$ can be interpreted as the average value of the random variable. It is computed as the sum of multiplying each value in $X$ with its probability:<br>\n",
    "$$E(X) = \\sum_{x \\in X} x * P(X=x)$$\n",
    "## Exercise 1: \n",
    "Calculate the expected value for the random variable $X$ (sum of the dice values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist vs. Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, we can make a hypothesis based on two approaches: __frequentist__ or __bayesian__. \n",
    "They mainly differ in that the bayesian approach uses a __prior probability__ to estimate the probability of new events while the frequentist approach does not assume a prior probability that differs in each case.\n",
    "\n",
    "<br> For example, let's assume that we are playing a game where we roll a dice and the person that rolls the highest number wins. Now, we know that each number has a probability of $P = \\frac{1}{6}$. But what if we know that the other player has a history of cheating? The __bayesian__ inference would make use of that knowledge and include the probability that the die of the other player is unfair. The __frequentist__ inference on the other hand would still assume the same \n",
    "probability regardless of our knowledge of the other player.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Central Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a probability distribution, we can calculate many useful metrics to describe the data. Some of them can be grouped as __measures of central tendency__, which are single values that describe the center of the data.\n",
    "\n",
    "## Mean\n",
    "The most common measure of central tendency is the __mean $\\bar{x}$__, the sum of all values divided by the number of values $n$:\n",
    "$$\\bar{x} = \\frac{\\sum x}{n}$$\n",
    "While it is a very popular measure, it is also very sensitive to __outliers__. Outliers are data points that are very far from the other values in a dataset. For example if we measure the height of people in a population, which are within a general range of 1.50m - 1.90m, we might record a value of a person with height 2.20m. This single case will heavily influence the mean of our dataset, which we should consider when choosing a measure of central tendency.\n",
    "\n",
    "## Median\n",
    "If we sort our data values, the __median__ is simply the middle value that separates the data into two equal-sized halves. This works fine, if we have 11 data points as the median will split the sides into 5 data points each. With an even number of data points, we would take the two middle values and average them. The advantage of the median is that it is not affected by outliers.\n",
    "\n",
    "## Mode\n",
    "The __mode__ is the value that occurs most frequently, i.e. the most popular value. It can tell us a lot about the data, but we should interpret it with caution. Particularly when there is more than one most frequent value, it is difficult to choose one as the mode. It is also not a reliable measure of central tendency if it is far away from the range of the remaining values.\n",
    "\n",
    "## Histograms and skewed distributions\n",
    "In addition to calculating measures that describe the data, visualising the data is also helpful to understand it better. Therefore, we can use __histograms__, which are a graphical representation of the frequency distribution underlying the data. __Histograms__ are bar charts with the possible data values on the x-axis and their respective frequency on the y-axis. We can easily read the __mode__ from a histogram by identifying the highest bar.<br>\n",
    "Histograms tell us a lot about the data. The shape of the following histogram describes a probability distribution that we will encounter often, namely the normal distribution. Here, the most frequent values are in the center, where we also find the mean, median and mode. The less likely values wander off equally to both sides. <br>\n",
    "<img src=\"images/bell-shaped-histogram.jpg\" align=\"center\"/> <br>\n",
    "Histogram of a normal distribution. ([Source](https://www.mathbootcamps.com/common-shapes-of-distributions/))<br>\n",
    "However, our data will not always a normal distribution. Below we can see __skewed distributions__.  The characteristiscs of a skewed distribution is its tail where the data clusters on one side. We speak of a __positively skewed distribution__, if the tail is on the positive side and the values become larger. We speak of a __negatively skewed distribution__, if the tail is on the negative side and the values become smaller. In a skewed distribution, the mean will move more in the direction of the skew. <br>\n",
    "<img src=\"images/skewed-left-histogram.jpg\" align=\"center\"/> <br>\n",
    "Histogram of a negatively skewed distribution. ([Source](https://www.mathbootcamps.com/common-shapes-of-distributions/))<br>\n",
    "<img src=\"images/skewed-right-histogram.jpg\" align=\"center\"/> <br>\n",
    "Histogram of a positively skewed distribution. ([Source](https://www.mathbootcamps.com/common-shapes-of-distributions/))<br>\n",
    "\n",
    "## Exercise 2\n",
    "Calculate the mean, median and mode of the height dataset. What would the histogram of a uniform distribution look like?<br> \n",
    "\n",
    "| ID | 1 | 2 | 3 | 4 | 5 | 6 | 7Â | 8 | 9 | 10 | 11 | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Height | 1.64 | 1.71 | 1.71 | 1.5 | 2.15 | 1.53 | 1.85 | 1.7 | 1.85 | 1.85 | 1.77 |"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While measures of central tendency describe the center of the data, we are also interested in how the rest of the data is scattered around the center. Therefore, we can use the following __measures of dispersion__:\n",
    "\n",
    "## Range\n",
    "As a first statistic, we should look at the __range__ of our values. The range is simply the difference between the maximum and minimum value. It is specifically useful to consider the range when adding new data as we can detect outliers, if they lie outside of the range.\n",
    "$$ \\text{Range} = Max - Min $$\n",
    "\n",
    "## Quartiles\n",
    "Similar to how the median divides the data into 2 halves, __quartiles__ divide the data into 4 quarters. You might also come across different denotations. The __first quartile__ corresponds to the __median__ of the lower half of the dataset. It is also known as the __25th percentile__ or __lower quartile__ and often abbreviated as __$Q_1$__. Similarly, the __upper quartile__ or __third quartile__ (__$Q_3$__) corresponds to the __75th percentile__ and denotes the median of the upper half of the dataset.\n",
    "\n",
    "## Interquartile range\n",
    "Unsurprisingly,  the __interquartile range__ corresponds to the range of values between the lower and upper quartile. While the __range__ over the entire dataset is the difference between the overall max and min values, the interquartile range denotes the difference between the the quartiles $Q_1$ and $Q_3$.\n",
    "$$ \\text{Interquartile range} = Q_3 - Q_1$$\n",
    "The interquartile range is a way to describe the data values without being skewed by outliers as they are not included here unlike in the overall range.\n",
    "<br>\n",
    "<img src=\"images/quartiles.png\" width=350 height=400 align=\"center\"/> <br>\n",
    "Quartiles and interquartile range within a normal probability distribution. ([Source](https://www.researchgate.net/figure/Relationship-of-quartiles-and-inter-quartile-range-Legends-Q-1-first-quartile-Q-3_fig2_324532937))<br>\n",
    "\n",
    "## Variance\n",
    "The __variance $\\sigma^2$__ measures how spread out from the mean the overall data is. Therefore, we first have to calculate the __mean $\\mu$__ and subtract from each value. In order to ensure positivity, we square the resulting values and then sum them together. This results in the __sum of squares__. To derive the __variance__ from the sum of squares, we simpy divide by the number of data points $n$.\n",
    "$$ \\sigma^2 = \\frac{(\\sum x - \\mu)^2}{n} $$\n",
    "A large variance indicates that the values are spread out far. By squaring the values, large values such as outliers will affect the variance a lot. It is also difficult to relate the variance to the data as the variance is based on squared values and therefore no longer in the same unit as the original data.\n",
    "\n",
    "## Standard deviation\n",
    "The __standard deviation $\\sigma$__ alleviates this problem by taking the square-root of the variance.\n",
    "$$ \\sigma = \\sqrt{\\frac{(\\sum x - \\mu)^2}{n}} $$\n",
    "\n",
    "## Exercise 3\n",
    "Determine the quartiles in the height dataset and compare range as well as interquartile range. Which measure which you choose for which purpose?<br>\n",
    "Calculate the variance and standard deviation of the height dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}