{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.1. Continuous Probability Distributions\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Discrete vs. Continuous Probability Distributions](#Discrete-vs.-Continous-Distributions)\n",
    "- [Probability Intervals & Histograms](#Probability-Intervals-&-Histograms)\n",
    "- [Probability Density](#Probability-Density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete vs. Continuous Probability Distributions\n",
    "\n",
    "So far we have covered probability distributions as functions that map probabilities to each of the possible outcomes of a random variable. The sum of all the probabilities of a probability distribution must equal 1 (as they include 100% of the possible outcomes). However, depending on the properties of the random variable we are looking at, the mathematical tools that we looked at before cannot directly be applied.\n",
    "\n",
    "So far, we have only considered discrete random variables, where there is a __finite, countable number of possible outcomes__. This means that we can assign probabilities to each of the outcomes.\n",
    "\n",
    "However, not all all random variables follow a _discrete_ distribution. What if I chose to investigate a random variable $X$, representing the height of a randomly selected student in a classroom. If I have a perfect ruler that measures with perfect accuracy, how many possible heights are there that can be observed? While perhaps not intuitive at first, the answer is that there are __infinitely many options__. \n",
    "\n",
    "This is not necessarily to do with treating a height of 0cm or a height of $\\infty$cm as possibilities. One way to look at this is that if I pick any two possible heights, there is always a possible height in between:\n",
    "- If I pick the heights 169cm and 170cm, someone can still be 169.5cm\n",
    "- If I pick the heights 169cm and 169.5cm, someone can still be 169.25cm\n",
    "- If I pick the heights 169cm and 169.25cm, someone can still be 169.125cm\n",
    "- If I pick the heights 169cm and 169.125cm, someone can still be 169.0625cm\n",
    "- $\\vdots$\n",
    "\n",
    "And we can carry out this process indefinitely. This means that even within a small range, we have infinitely many possible values our random variable can take. This is known as a __continuous random variable__, and follows a __continuous probability distribution__, where there is an __infinite, uncountable number of possible outcomes__. One more visual approach to be able to distinguish the two random variables is shown in the diagram below:\n",
    "<img src=\"images/jumps.png\" style=\"display:block; margin-left:auto; margin-right:auto; width:50%\"/>\n",
    "\n",
    "From this diagram it becomes clearer that a continuous random variable can be thought of as a continuous, smooth, connected range of possible values, whereas for a discrete random variable, we have a countable set of possible 'points' or outcomes. If we go back to the example of our dart throw, our random variable had four possible outcomes: $X \\in \\{1, 2, 3, 4\\}$. If we pick two possible outcomes, let's say 1 and 2, we see that there are no possible outcomes in between. This further confirms that it follows a discrete distribution.\n",
    "\n",
    "\n",
    "But how does the distinction between discrete and continuous random variables affect the tools we apply to given problems? Well, we saw from the beginning that the sum of the probabilities of all outcomes must equal 1. However, in this case, there are infinitely many possible outcomes. What kind of issue does this lead to?\n",
    "\n",
    "\n",
    "Since the sum of infinitely many positive numbers, no matter how big, is also infinity. Therefore, the individual probability of each outcome must be zero. But wait! Didn't we say the sum of the probabilities must be one? Then how does the sum of infinitely many zeros equal 1?!\n",
    "\n",
    "<img src=\"https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/160/facebook/105/shocked-face-with-exploding-head_1f92f.png\" style=\"display:block; margin-left:auto; margin-right:auto; width:20%\"/>\n",
    "\n",
    "This means that __there are inherent differences in the representation of continuous and discrete probability distributions.__ With continuous random variables, it does not make sense to assign a probability to a single outcome. If we cannot look at the likelihood of individual possible outcomes, how do we study them in the context of probability?\n",
    "\n",
    "One way to analyse the likelihood of values of continuous random variables is that instead of looking at individual values, we consider the __likehood of a value being within a given range or interval__. This leads us to a crucial tool to analyse continuous probability distributions: the __histogram__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Intervals & Histograms\n",
    "\n",
    "In the context of continuous probability distributions, we can construct a __histogram__, where we split the range of observed values into consecutive, non-overlapping intervals, referred to as __bins__ or __buckets__, measure the number of times a data point falls into each given bin, and plot this in a bar chart. The number of times an event occurs is known as its __frequency__. If we think of our data points as lying along a given number line, we can understand the concept of a histogram as follows:\n",
    "\n",
    "<img src=\"https://plot.ly/static/img/literacy/fig1.gif\" />\n",
    "<img src=\"https://plot.ly/static/img/literacy/fig2.gif\" />\n",
    "\n",
    "Let us now try to apply the concept of a histogram to the example of the height of students in a classroom. Below, we create a function that generates a given number of data points for this example, then use our own histogram generator based on Plotly Bar Charts. We will discuss what _normal_ means in more detail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import utils\n",
    "\n",
    "def class_heights(n):\n",
    "    return np.random.normal(175, 4, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = class_heights(n)\n",
    "bins, frequencies = utils.get_freq_data(x_lower=160, x_upper=190, data=x, bin_width=5)\n",
    "fig = utils.get_hist(bins=bins, frequencies=frequencies)\n",
    "fig.update_layout(\n",
    "    title_text='Height Distribution', # title of plot\n",
    "    xaxis_title_text='Height(cm)', # xaxis label\n",
    "    yaxis_title_text='Count (Frequency)', # yaxis label\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted above the histogram for the height distribution of a sample of 100 students, with a constant bin width of 5cm throughout. Some of you may be thinking: \"How does this weird looking thing relate to probability?\". Well, from our initial definition, probability is \"the number of wanted outcomes divide by the number of total outcomes\". Therefore, if we divide the frequency of each bin by the total number of outcomes, we get the __relative frequency__, which is the probability of a data point falling within the given bin.\n",
    "\n",
    "In our example, if we divide the number of students in each bin by 100 (total number of students), we get the following histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_data(x_lower, x_upper, data, bin_width):\n",
    "    bins, frequencies = utils.get_freq_data(x_lower, x_upper, data, bin_width)\n",
    "    probabilities = frequencies/np.sum(frequencies) ##\n",
    "    return bins, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, probabilities = get_prob_data(160, 190, x, 5)\n",
    "fig = utils.get_hist(bins, probabilities)\n",
    "fig.update_layout(\n",
    "    title_text='Height Distribution', # title of plot\n",
    "    xaxis_title_text='Height(cm)', # xaxis label\n",
    "    yaxis_title_text='Probability', # yaxis label\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that these are the correct probabilities corresponding to the different intervals by checking that the sum of all probabilities is 1, just as for the discrete case originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sum = np.sum(probabilities)\n",
    "print(prob_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the answer computed by Python is not exactly 1. The reason for this is known as numerical underflow, which can lead to small deviations from the result of floating point operations. We can check that the result we got is 1 with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isclose(1, prob_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now given set of data points sampled from a continuous random variable, we can describe the probability of given intervals using histograms. Therefore, this histogram is an __estimate of the probability distribution of the random variable we are investigating__. This is why histograms are an incredibly useful tool for understanding the underlying distribution of data we have sampled. Just as when we dealt with probability, we can calculate the probability of a wider interval by adding the probability of different intervals. For instance:\n",
    "\n",
    "$$P(170<X\\leq180) = P(170<X\\leq175) + P(175<X\\leq180)$$\n",
    "$$P(160<X\\leq180) = P(160<X\\leq165) + P(165<X\\leq170) + P(170<X\\leq175) + P(175<X\\leq180)$$\n",
    "\n",
    "\n",
    "However, this is not a perfect way to analyse probability. Why do you think that is?\n",
    "\n",
    "Well, not only are we only __estimating__ the probability distribution given a dataset, but we are turning a continuous random variable into a discrete random variable by grouping a range of values together. It is now discrete since if we look at different bins of a histogram, there are no bins in between. For instance, for the plot above, there are no bins between (170-175cm) and (175-180)cm. This means that __when the size of the bins are too large, we are losing a lot of information about the underlying distribution!__ For example, by looking at the above histogram, we have no way of differentiating between the probability distribution of measuring 171cm or 173cm, since they are in the same bin. We also have no way of measuring _all_ possible intervals. For example, we are unable to measure the $P(173<X<176)$. How do we go about fixing these problems?\n",
    "\n",
    "\n",
    "Well, if the problem occurs when the bins are too large, we can just make bins smaller right? This may have been unfeasible centuries ago, but with the power of programming we can make our bin width SIGNIFICANTLY small. However, as we saw above, since this is an estimate, the more data the more the estimate approaches its true value. This is even more relevant when we decrease bin width, as this means we have _more bins,_ meaning we need more data _per bin._\n",
    "\n",
    "Let us assume that we are sampling 100,000 students, meaning the probability distribution estimate tends to the true probability distribution, then see the effect of decreasing bin width on the quality of our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = class_heights(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "utils.visualize_prob_hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we approach a reasonably large number of bins, the probability of each bin approaches zero, as we see in the histograms above. This is because, as we the bin width gets closer and closer to 0, the more the 'interval' we are looking at starts behaving like a 'point'. However, we can see that even though individual probabilities get smaller as we decrease the width our intervals, __the probabilities remain the same relative to each other__. This is reflected by how the _shape_ of the plots remain relatively similar. How can we use this finding to better describe the probability distribution of this continuous random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density\n",
    "\n",
    "The optimal solution to this problem is to _normalize_ our probabilties with respect to the length of their interval (bin width), which gives us what is known as the __probability density__ of that interval, given by the equation below:\n",
    "\n",
    "$$ \\text{Probability Density} = \\frac{\\text{Interval Probability}}{\\text{Interval Width}}$$\n",
    "\n",
    "This explains why most histograms will use the same bin width for all bins. When we do this, we get the histograms shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_data(x_lower, x_upper, data, bin_width):\n",
    "    bins, probabilities = get_prob_data(x_lower, x_upper, data, bin_width)\n",
    "    prob_densities = probabilities/bin_width ##\n",
    "    return bins, prob_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_density_hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, even at incredibly small bin widths, we are still getting plots on the same scale while preserving the relevant information regarding the underlying probability distribution. In the case of our example, probability density is measured as __probability per cm__. \n",
    "\n",
    "The most important property of probability density is that as the interval width gets incredibly small (pretty much 0), such that each interval can be approximated by a point, we can still measure probability density (unlike probability)! Knowing the interval width (bin width), we can also easily calculate the probability of an interval given its probability density by rearranging the equation above:\n",
    "\n",
    "$$ \\text{Interval Probability}= \\text{Probability Density} \\times \\text{Interval Width}$$\n",
    "\n",
    "Coincidentally, if we look at our probability density histograms, we see that this corresponds to the area of the bar corresponding to that interval, as shown in the diagram below:\n",
    "\n",
    "<img src=\"images/area.png\" style=\"display:block; margin-left:auto; margin-right:auto; width:50%\"/>\n",
    "\n",
    "Therefore, if we have an estimate of the probability density distribution, we can estimate the probability of any given interval. In theory, if we know the probability density of every single point, we can calculate the exact probability of any given interval. While in the real-world, we will never know the _exact_ probability density distribution of a continuous random variable, we can model them with what are known as __probability density functions (PDFs)__. These are functions that define the probability density of every single possible outcome of a continuous random variable.\n",
    "\n",
    "Any function can be treated as a probability density function as long as it meets the two following criteria:\n",
    "- The probabilities described by the PDF must equal to 1\n",
    "- All probability densities must be positive\n",
    "\n",
    "The concept of PDFs is generally quite abstract, so it becomes more intuitive to think of them as __probability density histograms with lots of data and really, really small interval widths.__ For example, let us assume that a continuous random variable, X, follows the PDF given below:\n",
    "\n",
    "$$f(x) = e^{-x}$$\n",
    "\n",
    "Let us assume that we were able to collect 10,000 samples from this continuous random variable. Below, we plot this function with the respective histogram at different interval widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data points to plot the given function\n",
    "def exp_pdf(x):\n",
    "    return np.exp(-x) ##\n",
    "x_true = np.linspace(0, 5, 1000)\n",
    "f_true = exp_pdf(x_true)\n",
    "\n",
    "# Sampling the exponential distribution given above\n",
    "n = 100000\n",
    "x_sample = np.random.exponential(size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_exp_approx(x_sample, x_true, f_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, for a reasonably large set of data points, as we make our bin width smaller and smaller, we approach the true PDF. Now that you have this intuitive understanding of PDFs, how can we calculate the probability of an interval of values for our continuous random variable?\n",
    "\n",
    "Since the area of the rectangular bar representing the interval is the probability of that interval, we just __add the probability of all the intermediary intervals__. As the bin widths get smaller and smaller, meaning we get closer to the true PDF, this becomes the area underneath the curve. Therefore, for any PDF, __we can calculate the probability of any interval as the area underneath the curve within that interval.__ Below we show a function that computes the probability of an interval given the true PDF, which allows us to tweak the bin width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Challenge\n",
    "def prob_estimate(x_lower, x_upper, pdf_func, bin_width):\n",
    "    area_count = 0\n",
    "    current_x = x_lower\n",
    "    while current_x < x_upper:\n",
    "        area_count+= pdf_func(current_x)*bin_width ## adding the area of the current interval\n",
    "        current_x += bin_width ##\n",
    "    return area_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_exp_interval(x_true, f_true, exp_pdf)"
   ]
  },
  {
   "source": [
    "As you can see, the probability estimation gets better and better for small bin widths, approaching the true probability of $0.318$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this histogram approximation allows us to compute the probability of an interval of values with accuracy increasing as we decrease the interval width. Therefore, if you have to investigate the probability distribution of any continuous random variable, you can either:\n",
    "- Estimate the probability/probability density of individual intervals of values by sampling the variable using histograms for the data (e.g. how we collected data from a large classroom)\n",
    "- Use functions to model the probability density of a continuous random variable at every point (as we did when looking at the exponential distribution)\n",
    "\n",
    "As we can expect, since the sum of the probabilities of all possible outcomes must be one, __the entire area under the curve of a pdf must equal 1__. While we cannot show this exactly for $f(x) = e^{-x}$, as it goes on forever, we can estimate it with a large enough number (in this case 10,000), as shown below for a bin width of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_estimate(0, 10000, exp_pdf, 0.01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}